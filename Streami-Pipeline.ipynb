{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4361a-1cd3-4341-b5fd-df376000f117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.metrics import Metrics\n",
    "import logging\n",
    "import google.cloud.logging\n",
    "\n",
    "# Initialize Google Cloud Logging\n",
    "logging_client = google.cloud.logging.Client()\n",
    "logging_client.setup_logging()\n",
    "logger = logging.getLogger('stream-pipeline-logger')\n",
    "\n",
    "INPUT_TOPIC = \"projects/model-arcadia-440702-q1/topics/my-topic\"\n",
    "\n",
    "# Logging helper function\n",
    "def log_pipeline_step(step, message, level='INFO'):\n",
    "    if level == 'INFO':\n",
    "        logger.info(f\"Step: {step}, Message: {message}\")\n",
    "    elif level == 'ERROR':\n",
    "        logger.error(f\"Step: {step}, Error: {message}\")\n",
    "    elif level == 'WARNING':\n",
    "        logger.warning(f\"Step: {step}, Warning: {message}\")\n",
    "\n",
    "# Define Beam options\n",
    "beam_options = PipelineOptions(\n",
    "    streaming=True,\n",
    "    runner='DataflowRunner',\n",
    "    project='model-arcadia-440702-q1',\n",
    "    job_name='stream-dataflow-job-2',\n",
    "    region='us-east1',\n",
    "    temp_location='gs://test-bkt-123123123/temp23'\n",
    ")\n",
    "\n",
    "# Data parsing with validation and error handling\n",
    "class ParseDataFn(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        # Initialize metrics\n",
    "        self.success_count = Metrics.counter('main', 'successful-records')\n",
    "        self.failure_count = Metrics.counter('main', 'failed-records')\n",
    "        self.processed_bytes = Metrics.distribution('main', 'processed-bytes')\n",
    "\n",
    "    def process(self, element):\n",
    "        try:\n",
    "            # Decode element from bytes to string\n",
    "            element = element.decode('utf-8')\n",
    "            fields = element.split(\",\")\n",
    "\n",
    "            HEADERS = [\"id\", \"factor\", \"code\", \"time\", \"name\"]\n",
    "\n",
    "            # Validate the number of fields\n",
    "            if len(fields) != len(HEADERS):\n",
    "                self.failure_count.inc()\n",
    "                log_pipeline_step('ParseData', f\"Invalid record: {element}\", level='WARNING')\n",
    "                return  # Skip invalid record\n",
    "\n",
    "            # Create a dictionary from the fields\n",
    "            record = dict(zip(HEADERS, fields))\n",
    "\n",
    "            # Validate the content of the fields\n",
    "            if not record[\"id\"] or not record[\"code\"]:\n",
    "                self.failure_count.inc()\n",
    "                log_pipeline_step('ParseData', f\"Missing required fields in record: {element}\", level='WARNING')\n",
    "                return  # Skip invalid record\n",
    "\n",
    "            # Track the size of the processed element\n",
    "            self.processed_bytes.update(len(element))\n",
    "\n",
    "            self.success_count.inc()\n",
    "            log_pipeline_step('ParseData', f\"Successfully processed record: {record}\", level='INFO')\n",
    "            yield record\n",
    "\n",
    "        except Exception as e:\n",
    "            self.failure_count.inc()\n",
    "            log_pipeline_step('ParseData', f\"Error processing record: {element}, Error: {str(e)}\", level='ERROR')\n",
    "\n",
    "# Pipeline object\n",
    "with beam.Pipeline(options=beam_options) as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Read from Pub/Sub topic\" >> beam.io.ReadFromPubSub(topic=INPUT_TOPIC)\n",
    "        | \"Parse and Validate Data\" >> beam.ParDo(ParseDataFn())\n",
    "        | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "            table=\"model-arcadia-440702-q1.silver_dataset.streamTable\",\n",
    "            schema='id:string, factor:string, code:string, time:string, name:string',\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            custom_gcs_temp_location=\"gs://test-bkt-123123123/temp22/\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a04a5-8516-4431-a763-0ce278a45354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "apache-beam-2.60.0",
   "name": ".m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m126"
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.60.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.60.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
